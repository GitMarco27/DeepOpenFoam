import logging
import math
import random

import gym
from IPython.core.display import clear_output
from gym.spaces import Box, Discrete
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

# This file contains all classes based on environment based on latent parameters generated by an autoencoder.
# The LDEnv class describes the operations that all subclasses derive from it
from . import reward_functions
from inspect import getmembers, isfunction

def get_all_rewards_functions():
    list_reward_fucntions_with_keys = getmembers(reward_functions, isfunction)

    possible_reward_functions = {}
    for element in list_reward_fucntions_with_keys:
        possible_reward_functions[element[0]] = element[1]
    return possible_reward_functions


class LDEnv(gym.Env):


    # starting geometry index
    current_index = 0

    # max number of steps per episode
    num_steps_max = None

    num_step = 0

    # The state is composed of the latent parameters and other global parameters
    _state = None

    # boolenan variable that tells if the episode is concluded
    _episode_ended = False

    # I also save the current geometry that I use later in the render
    _current_geom = 1

    _starting_geom = 1

    # save the old action
    _old_action = None

    # Size of observation and action spaces
    action_space = None
    observation_space = None

    ref_value = 10000

    # index used for test a specific geometry
    select_geom_index = None

    def __init__(self, models, data_env, rl_config):
        super(LDEnv, self).__init__()

        plt.style.use(rl_config['style_matplotlib'])


        # Load autoencoders and prediction models.
        self.models = models

        # I get the environment variables from the configuration file
        self.num_steps_max = rl_config['steps_per_episode']

        # In date env we find several dait:
        # 1. Latent data
        # 2. Original data
        self.data_env = data_env

        possible_reward_functions = get_all_rewards_functions()
        self.get_eff_reward = possible_reward_functions[rl_config['type_rewards']]

        # get number of latent parameters
        self._number_of_latent_parameters = self.data_env['cod'].shape[1]

        # get the max value of action
        self.delta_value = rl_config['delta_value']

        # Calculates parameters for normalization as maximum and minimum value for each of latents and then adds them
        # into data_env dict
        min_values_latent, max_values_latent, min_gv, max_gv = self.calc_min_max_of_state()
        margin_latent_extremes = rl_config['margin_latent_extremes']
        self.data_env['min_values_latent'] = min_values_latent -margin_latent_extremes
        self.data_env['max_values_latent'] = max_values_latent + margin_latent_extremes
        self.data_env['min_gv'] = min_gv
        self.data_env['max_gv'] = max_gv

        # Observation space
        min_value_observation = np.array(self.data_env['min_values_latent'].tolist() + self.data_env['min_gv'].tolist())
        max_value_observation = np.array(self.data_env['max_values_latent'].tolist() + self.data_env['max_gv'].tolist())

        self.observation_space = Box(low=min_value_observation, high=max_value_observation, dtype=np.float32)

        self._state = self.observation_space.sample()


    def get_reward(self, state, ref_value, episode_ended:bool= False):
        eff_reward =self.get_eff_reward(state, ref_value, episode_ended=episode_ended )
        return eff_reward

    # function to decocde
    def decode(self):
        pass

    # Function used to predict the global variables
    def pred_global_variables(self):
        pass

    # Function used to calculate the maximum and minimum for each of the latent params
    def calc_min_max_of_state(self):
        pd_all_cod = pd.DataFrame(self.data_env['cod'])
        min_latent = pd_all_cod.min().to_numpy()
        max_latent = pd_all_cod.max().to_numpy()

        pd_all_gv = pd.DataFrame(self.data_env['origin_global_variables'])
        min_gv = pd_all_gv.min().to_numpy()
        max_gv = pd_all_gv.max().to_numpy()

        return min_latent.reshape(-1), max_latent.reshape(-1), min_gv.reshape(-1), max_gv.reshape(-1),

    def _get_obs(self):
        return np.array(self._state)

    def get_latent_data_from_state(self):
        return self._get_obs()[:self._number_of_latent_parameters]

    def get_global_variable_from_state(self):
        return self._get_obs()[self._number_of_latent_parameters:]

    def get_ref_value(self):
        return self._state[-1]

    def reset(self):
        """
        Important: the observation must be a numpy array
        :return: (np.array)
        """
        self._episode_ended = False

        if self.select_geom_index is not None:
            self.current_index = self.select_geom_index
        else:
            # I randomly choose a new geometry for this episode
            self.current_index = random.randint(0, self.data_env['cod'].shape[0] - 1)

        # Extract current latents
        current_laten_params = self.data_env['cod'][self.current_index]

        self._state = current_laten_params.reshape(-1).tolist()

        # Calculate global variables
        global_variables = self.pred_global_variables()

        self._state.extend(global_variables)

        self.num_step = 0

        return self._get_obs()

    def update_latent(self, current_laten_params, action):
        return None

    def step(self, action):
        self.num_step += 1

        if self._episode_ended:
            return self.reset()

        # I sum the current latent parameters with actions
        current_laten_params = self.get_latent_data_from_state()

        new_latent_params = self.update_latent(current_laten_params, action)

        self._state = new_latent_params.reshape(-1).tolist()

        # calculate global variables and add them to the state
        global_variables = self.pred_global_variables()

        for variable in global_variables:
            self._state.append(variable)

        # Check if the episode is finished
        if self.num_step >= self.num_steps_max:
            self._episode_ended = True

        # calculate the reward
        reward = self.get_reward(self._state, self.ref_value, self._episode_ended)

        # current value
        current_value = self.get_ref_value()

        return self._get_obs(), reward, self._episode_ended, {'best_value_obtained': self.ref_value,
                                                              'current_value': current_value}

    def render(self,  mode="human"):
        pass




class DiscreteActionLDEnv(LDEnv):
    def __init__(self, models, data_env, rl_config):
        super(DiscreteActionLDEnv, self).__init__(models, data_env, rl_config)

        # the actions are divided in this way
        # 0 I do nothing
        # 1-> Number of latent parameters: I add to the latent amount
        # Number of latent parameters+1-> End:  subtract from the latent equivalent
        self.action_space = Discrete(2 * rl_config['delta_value'] * self._number_of_latent_parameters + 1)


    def update_latent(self, current_laten_params, action):
        if action == 0:
            return current_laten_params

        value_to_sum = math.floor(action / 2048) + 1

        if action > 2 * self._number_of_latent_parameters + 1:
            action = action - ((2 * self._number_of_latent_parameters) * (value_to_sum - 1))

        self._old_action = np.zeros(current_laten_params.shape)

        if action > self._number_of_latent_parameters:
            self._old_action[int(action - 1024 - 1)] = -value_to_sum
            current_laten_params[int(action - 1024 - 1)] -= (value_to_sum / 100) * current_laten_params[
                int(action - 1024 - 1)]
        elif action > 0:
            self._old_action[int(action - 1)] = value_to_sum
            current_laten_params[int(action - 1)] += (value_to_sum / 100) * current_laten_params[int(action - 1)]

        new_latent_params = current_laten_params
        return new_latent_params


class BoxActionLDEnv(LDEnv):
    def __init__(self, models, data_env, rl_config):
        super(BoxActionLDEnv, self).__init__(models, data_env, rl_config)

        delta_action = (self.delta_value/100)*abs(self.data_env['max_values_latent']-self.data_env['min_values_latent'])
        # Define action and observation space
        # They must be gym.spaces objects
        # self.action_space = spaces.Box(low=-np.array(1024), high=np.array(1024), dtype=np.int64)
        self.action_space = Box(low=-delta_action, high=delta_action,
                                dtype=np.float32)

    def update_latent(self, current_laten_params, action):
        self._old_action = action
        current_laten_params += action
        return current_laten_params

class AirFoilLDenv(BoxActionLDEnv):
    def __init__(self, models, data_env, rl_config):
        super(AirFoilLDenv, self).__init__(models, data_env, rl_config)

        # load parameters for fitting
        self.fit_params = rl_config['fit_params']

        # List reward
        _episode_rewards = {'Cl_reward': [],
                            'Cd_reward': [],
                            'Fitting_reward': [],
                            'Sum_reward': []}

    def fit_current_geom(self, current_geom):
        geom = current_geom
        geom = geom[geom[:,0]>self.fit_params['th_x']]
        fitting_curve, fitting_error = self.models['fit_geom'](geom, order=self.fit_params['order'],
                                                   sub_geom=self.fit_params['sub_geom'])

        fitting_reward = (1/fitting_error) * float(self.fit_params['alpha'])

        abnormal_points, edges_anomalous_region = self.models['calc_y_distance'](fitting_curve, self.fit_params)
        return fitting_curve, fitting_reward, abnormal_points, edges_anomalous_region

    def get_reward(self, state, ref_value, episode_ended:bool= False):
        eff_reward = super(AirFoilLDenv, self).get_reward(state, ref_value, episode_ended=episode_ended )

        if episode_ended:
            self._current_geom = self.decode()

            _, fitting_reward,abnormal_points, _  = self.fit_current_geom(self._current_geom[0])

            if abnormal_points:
                return 0
        else:
            fitting_reward = 0

        return eff_reward + fitting_reward

    def decode(self):
        models = self.models
        decoder = models['decoder']
        scaler_geom = models['scaler_geom']
        denorm_geom = models['denorm_geom']

        latent_param = self.get_latent_data_from_state()
        latent_param = np.expand_dims(latent_param, axis=0).reshape(1, -1)

        X_norm = decoder.predict(latent_param)

        X = denorm_geom(X_norm, scaler_geom['min_y'], scaler_geom['max_y'])
        return X

    def pred_global_variables(self):
        models = self.models
        model_reg = models['reg']
        scaler_globals = models['scaler_globals']

        # Calcolo delle variabili globali
        latent_param = self.get_latent_data_from_state()
        latent_param = np.expand_dims(latent_param, axis=0).reshape(1, -1)
        global_variables = model_reg.predict(latent_param)  # .reshape(-1).tolist()

        global_variables = scaler_globals.inverse_transform(global_variables).reshape(-1).tolist()

        if global_variables[1] == 0.:
            global_variables[1] = 0.00001

        return global_variables

    def reset(self):
        """
        Important: the observation must be a numpy array
        :return: (np.array)
        """
        super(AirFoilLDenv, self).reset()

        self.ref_value = self.get_ref_value()

        self._starting_geom = np.expand_dims(self.data_env['origin_geom'][self.current_index], axis=0)
        self._starting_geom = self.models['denorm_geom'](self._starting_geom, self.models['scaler_geom']['min_y'],
                                                         self.models['scaler_geom']['max_y'])
        self._episode_rewards = {'Cl_reward': [],
                                 'Cd_reward': [],
                                 'Fitting_reward': [],
                                 'Sum_reward': []}
        return self._get_obs()

    def get_ref_value(self):
        Cl = self._state[-2]
        Cd = self._state[-1]

        current_value = Cl / Cd
        return current_value

    def render(self, mode="human"):
        # Starting Geom
        starting_geom = self._starting_geom

        # Current Geom
        self._current_geom = self.decode()

        fitting_curve, fitting_reward, abnormal_points, edges_anomalous_region = self.fit_current_geom(self._current_geom[0])

        gv = self.pred_global_variables()
        # salve current reward
        self._episode_rewards['Cl_reward'].append(10 * gv[0])
        self._episode_rewards['Cd_reward'].append(1 / gv[1])
        self._episode_rewards['Fitting_reward'].append(fitting_reward)
        self._episode_rewards['Sum_reward'].append(fitting_reward+self._episode_rewards['Cl_reward'][-1]+self._episode_rewards['Cd_reward'][-1])

        clear_output(wait=True)

        starting_value = self.ref_value

        Cl = self._state[-2]
        Cd = self._state[-1]

        current_value = Cl / Cd

        current_value = current_value
        delta = round(current_value - starting_value, 4)

        if delta < 0:
            color_points = 'red'
        elif delta > 0:
            color_points = 'green'
        else:
            color_points = 'blue'

        colors = np.array(['green'] * self._old_action.shape[0])
        colors[self._old_action < 0] = 'red'
        colors[self._old_action == 0] = 'blue'

        fig = plt.figure(figsize=(15, 15))
        fig.add_subplot(311)
        plt.title(
            f'Env Step {self.num_step}:     starting_ratio : {round(starting_value, 4)} -----> current_ratio {round(current_value, 4)} = Delta ratio: {delta}\n'
            f'Cl: {round(Cl, 4)}    Cd {round(Cd, 4)} ',
            fontsize=16)
        plt.plot(starting_geom[0][:, 0], starting_geom[0][:, 1], c='blue')
        plt.fill(starting_geom[0][:, 0], starting_geom[0][:, 1], color='blue', alpha=0.2)

        self.models['plot_fit_curve'](fitting_curve, color_points)
        plt.scatter(self._current_geom[0][:, 0], self._current_geom[0][:, 1], c=color_points, s=15, edgecolor='k')

        if abnormal_points:
            plt.axvspan(edges_anomalous_region['min_x'], edges_anomalous_region['max_x'], facecolor='r', alpha=0.5)

        plt.ylim([-0.35, 0.35])
        plt.xlim([-0.02, 1.02])

        fig.add_subplot(312)
        plt.plot(self._episode_rewards['Cl_reward'], label='Cl_reward')
        plt.plot(self._episode_rewards['Cd_reward'], label='Cd_reward')
        plt.plot(self._episode_rewards['Fitting_reward'], label='Fitting_reward')
        plt.plot(self._episode_rewards['Sum_reward'], label='Sum_reward', color='C3')
        plt.legend(loc='upper left', prop={'size': 15})
        # plt.ylim([-10, 100])
        plt.title('Rewards')

        fig.add_subplot(325)
        plt.scatter(np.arange(len(self._state[:self._number_of_latent_parameters])),
                    self._state[:self._number_of_latent_parameters], c=colors)
        plt.plot(self.data_env['max_values_latent'])
        plt.title('Current latent parameters')

        fig.add_subplot(326)
        plt.scatter(np.arange(len(self._old_action)), self._old_action, c=colors)
        plt.title('Action')

        plt.show()


