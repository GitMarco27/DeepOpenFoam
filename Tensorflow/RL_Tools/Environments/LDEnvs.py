import logging
import math
import random

import gym
from IPython.core.display import clear_output
from gym.spaces import Box, Discrete
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

from .env_models_functions import pred_global_variables, decode

# This file contains all classes based on environment based on latent parameters generated by an autoencoder.
# The LDEnv class describes the operations that all subclasses derive from it
from . import reward_functions
from inspect import getmembers, isfunction

def get_all_rewards_functions():
    list_reward_fucntions_with_keys = getmembers(reward_functions, isfunction)

    possible_reward_functions = {}
    for element in list_reward_fucntions_with_keys:
        possible_reward_functions[element[0]] = element[1]
    return possible_reward_functions


class LDEnv(gym.Env):


    # starting geometry index
    current_index = 0

    # max number of steps per episode
    num_steps_max = None

    current_step = 0

    # The state is composed of the latent parameters and other global parameters
    _state = None

    # boolenan variable that tells if the episode is concluded
    _episode_ended = False

    # I also save the current geometry that I use later in the render
    _current_geom = 1

    _starting_geom = 1

    # save the old action
    _old_action = None

    # Size of observation and action spaces
    action_space = None
    observation_space = None

    ref_value = 10000

    # index used for test a specific geometry
    select_geom_index= None

    def __init__(self, models, data_env, rl_config):
        super(LDEnv, self).__init__()

        plt.style.use(rl_config['style_matplotlib'])


        # Load autoencoders and prediction models.
        self.models = models

        # I get the environment variables from the configuration file
        self.num_steps_max = rl_config['steps_per_episode']

        # In date env we find several dait:
        # 1. Latent data
        # 2. Original data
        self.data_env = data_env

        possible_reward_functions = get_all_rewards_functions()
        self.get_eff_reward = possible_reward_functions[rl_config['type_rewards']]

        # get number of latent parameters
        self._number_of_latent_parameters = self.data_env['cod'].shape[1]

        # get the max value of action
        self.delta_value = rl_config['delta_value']

        # Calculates parameters for normalization as maximum and minimum value for each of latents and then adds them
        # into data_env dict
        min_values_latent, max_values_latent, min_gv, max_gv = self.calc_min_max_of_state()
        margin_latent_extremes = rl_config['margin_latent_extremes']
        self.data_env['min_values_latent'] = min_values_latent -margin_latent_extremes
        self.data_env['max_values_latent'] = max_values_latent + margin_latent_extremes
        self.data_env['min_gv'] = min_gv
        self.data_env['max_gv'] = max_gv

        # Observation space
        min_value_observation = np.array(self.data_env['min_values_latent'].tolist()+self.data_env['min_gv'].tolist())
        max_value_observation = np.array(self.data_env['max_values_latent'].tolist()+self.data_env['max_gv'].tolist())

        self.observation_space = Box(low=min_value_observation, high=max_value_observation, dtype=np.float32)

        self._state = self.observation_space.sample()


    def get_reward(self, state, ref_value, episode_ended:bool= False):
        eff_reward =self.get_eff_reward(state, ref_value, episode_ended=episode_ended )
        return eff_reward

    # Function used to calculate the maximum and minimum for each of the latent params
    def calc_min_max_of_state(self):
        pd_all_cod = pd.DataFrame(self.data_env['cod'])
        min_latent = pd_all_cod.min().to_numpy()
        max_latent = pd_all_cod.max().to_numpy()

        pd_all_gv = pd.DataFrame(self.data_env['origin_global_variables'])
        min_gv = pd_all_gv.min().to_numpy()
        max_gv = pd_all_gv.max().to_numpy()

        return min_latent.reshape(-1), max_latent.reshape(-1), min_gv.reshape(-1) , max_gv.reshape(-1),

    def _get_obs(self):
        return np.array(self._state)

    def get_latent_data_from_state(self):
        return self._get_obs()[:self._number_of_latent_parameters]

    def get_global_variable_from_state(self):
        return self._get_obs()[self._number_of_latent_parameters:]

    def get_ref_value(self):
        return self._state[-1]

    def calculate_global_variabls(self, current_laten_params):
        global_variables = pred_global_variables(current_laten_params, self.models)
        return global_variables

    def reset(self):
        """
        Important: the observation must be a numpy array
        :return: (np.array)
        """
        self._episode_ended = False

        if self.select_geom_index is not None:
            self.current_index = self.select_geom_index
        else:
            # I randomly choose a new geometry for this episode
            self.current_index = random.randint(0, self.data_env['cod'].shape[0] - 1)

        # Extract current latents
        current_laten_params = self.data_env['cod'][self.current_index]

        self._state = current_laten_params.reshape(-1).tolist()

        # Calculate global variables
        global_variables = self.calculate_global_variabls(current_laten_params)

        self._state.extend(global_variables)

        self.num_step = 0

        return self._get_obs()

    def update_latent(self, current_laten_params, action):
        return None

    def step(self, action):
        self.num_step += 1

        if self._episode_ended:
            return self.reset()

        # I sum the current latent parameters with actions
        current_laten_params = self.get_latent_data_from_state()

        new_latent_params = self.update_latent(current_laten_params, action)

        self._state = new_latent_params.reshape(-1).tolist()

        # calculate global variables and add them to the state
        global_variables = self.calculate_global_variabls(current_laten_params)

        for variable in global_variables:
            self._state.append(variable)

        # Check if the episode is finished
        if self.num_step >= self.num_steps_max:
            self._episode_ended = True

        # calculate the reward
        reward = self.get_reward(self._state, self.ref_value, self._episode_ended)

        # current value
        current_value = self.get_ref_value()

        return self._get_obs(), reward, self._episode_ended, {'best_value_obtained': self.ref_value,
                                                              'current_value': current_value}

    def render(self,  mode="human"):
        pass




class DiscreteActionLDEnv(LDEnv):
    def __init__(self, models, data_env, rl_config):
        super(DiscreteActionLDEnv, self).__init__(models, data_env, rl_config)

        # the actions are divided in this way
        # 0 I do nothing
        # 1-> Number of latent parameters: I add to the latent amount
        # Number of latent parameters+1-> End:  subtract from the latent equivalent
        self.action_space = Discrete(2 * rl_config['delta_value'] * self._number_of_latent_parameters + 1)


    def update_latent(self, current_laten_params, action):
        if action == 0:
            return current_laten_params

        value_to_sum = math.floor(action / 2048) + 1

        if action > 2 * self._number_of_latent_parameters + 1:
            action = action - ((2 * self._number_of_latent_parameters) * (value_to_sum - 1))

        self._old_action = np.zeros(current_laten_params.shape)

        if action > self._number_of_latent_parameters:
            self._old_action[int(action - 1024 - 1)] = -value_to_sum
            current_laten_params[int(action - 1024 - 1)] -= (value_to_sum / 100) * current_laten_params[
                int(action - 1024 - 1)]
        elif action > 0:
            self._old_action[int(action - 1)] = value_to_sum
            current_laten_params[int(action - 1)] += (value_to_sum / 100) * current_laten_params[int(action - 1)]

        new_latent_params = current_laten_params
        return new_latent_params


class BoxActionLDEnv(LDEnv):
    def __init__(self, models, data_env, rl_config):
        super(BoxActionLDEnv, self).__init__(models, data_env, rl_config)

        delta_action = (self.delta_value/100)*abs(self.data_env['max_values_latent']-self.data_env['min_values_latent'])
        # Define action and observation space
        # They must be gym.spaces objects
        # self.action_space = spaces.Box(low=-np.array(1024), high=np.array(1024), dtype=np.int64)
        self.action_space = Box(low=-delta_action, high=delta_action,
                                dtype=np.float32)

    def update_latent(self, current_laten_params, action):
        self._old_action = action
        current_laten_params += action
        return current_laten_params

class AirFoilLDenv(BoxActionLDEnv):
    def __init__(self, models, data_env, rl_config):
        super(AirFoilLDenv, self).__init__(models, data_env, rl_config)

        # load parameters for fitting
        self.fit_params = rl_config['fit_params']

    def fit_current_geom(self, current_geom):
        geom = current_geom
        geom = geom[geom[:,0]>self.fit_params['th_x']]
        fitting_curve, fitting_error = self.models['fit_geom'](geom, order=self.fit_params['order'],
                                                   sub_geom=self.fit_params['sub_geom'])

        fitting_reward = (1/fitting_error) * float(self.fit_params['alpha'])

        return fitting_curve, fitting_reward

    def get_reward(self, state, ref_value, episode_ended:bool= False):
        eff_reward = super(AirFoilLDenv, self).get_reward(state, ref_value, episode_ended=episode_ended )

        if episode_ended:
            current_laten_params = self.get_latent_data_from_state()
            self._current_geom = decode(self.models, current_laten_params)

            _, fitting_reward = self.fit_current_geom(self._current_geom[0])
        else:
            fitting_reward = 0

        return eff_reward + fitting_reward

    def reset(self):
        """
        Important: the observation must be a numpy array
        :return: (np.array)
        """
        super(AirFoilLDenv, self).reset()

        self.ref_value = self.get_ref_value()

        self._starting_geom = np.expand_dims(self.data_env['origin_geom'][self.current_index], axis=0)
        self._starting_geom = self.models['denorm_geom'](self._starting_geom, self.models['scaler_geom']['min_y'],
                                                         self.models['scaler_geom']['max_y'])

        return self._get_obs()

    def get_ref_value(self):
        Cl = self._state[-2]
        Cd = self._state[-1]

        current_value = Cl / Cd
        return current_value

    def render(self,  mode="human"):
        # Starting Geom
        starting_geom = self._starting_geom

        # Current Geom
        current_laten_params = self.get_latent_data_from_state()
        self._current_geom = decode(self.models, current_laten_params)

        fitting_curve, fitting_reward = self.fit_current_geom(self._current_geom[0])

        clear_output(wait=True)

        starting_value =self.ref_value

        Cl = self._state[-2]
        Cd = self._state[-1]

        current_value = Cl / Cd

        current_value =current_value
        delta = round(current_value - starting_value, 4)

        if delta < 0:
            color_points = 'red'
        elif delta > 0:
            color_points = 'green'
        else:
            color_points = 'blue'

        colors = np.array(['green'] * self._old_action.shape[0])
        colors[self._old_action < 0] = 'red'
        colors[self._old_action == 0] = 'blue'

        plt.figure(figsize=(15, 15))
        plt.subplot(2, 1, 1)
        plt.title(
            f'Env: step {self.num_step}, current_value {round(current_value, 4)}, starting eff : {round(starting_value, 4)}, '
            f'Cl: {round(Cl, 4)}, Cd {round(Cd, 4)}, \nDelta eff: {delta} , Fitting Reward: {fitting_reward}', fontsize=16)
        plt.plot(starting_geom[0][:, 0], starting_geom[0][:, 1], c='blue')
        plt.fill(starting_geom[0][:, 0], starting_geom[0][:, 1], color='blue', alpha=0.2)

        self.models['plot_fit_curve'](fitting_curve, color_points)
        plt.scatter(self._current_geom[0][:, 0], self._current_geom[0][:, 1], c=color_points, s=15, edgecolor='k')
        plt.ylim([-0.35, 0.35])
        plt.xlim([-0.02, 1.02])

        plt.subplot(2, 2, 3)
        plt.scatter(np.arange(len(self._state[:self._number_of_latent_parameters])),
                         self._state[:self._number_of_latent_parameters], c=colors)
        plt.plot(self.data_env['max_values_latent'])
        plt.title('Current latent parameters')

        plt.subplot(2, 2, 4)
        plt.scatter(np.arange(len(self._old_action)), self._old_action, c=colors)
        plt.title('Action')
        plt.show()

